{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09cf0177-e7fd-4b6b-b2ac-1e2fff30d06f",
   "metadata": {},
   "source": [
    "# Lab 2: Automatic Differentiation (Autograd) - Demo\n",
    "\n",
    "This notebook demonstrates the automatic differentiation capabilities implemented in Lab 2.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Automatic Differentiation](#intro)\n",
    "2. [Basic Gradient Computation](#basic)\n",
    "3. [Chain Rule in Action](#chain)\n",
    "4. [Computational Graphs](#graphs)\n",
    "5. [Real-World Examples](#examples)\n",
    "6. [Gradient Checking](#checking)\n",
    "7. [Common Pitfalls](#pitfalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6616976-1b7b-46a6-8543-333fa59136f2",
   "metadata": {},
   "source": [
    "## 1. Introduction to Automatic Differentiation {#intro}\n",
    "\n",
    "Automatic differentiation (autograd) is the foundation of modern deep learning. It allows us to automatically compute gradients of complex functions, which is essential for training neural networks using gradient descent.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Computational Graph**: A directed acyclic graph (DAG) representing the computation\n",
    "- **Forward Pass**: Computing the function output\n",
    "- **Backward Pass**: Computing gradients using the chain rule\n",
    "- **Gradient Accumulation**: Summing gradients from multiple paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "196f7385-f891-4512-955b-3b6483ebe054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd module loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../../../')\n",
    "\n",
    "from semester1.lab2_autograd.autograd import Tensor, no_grad, check_gradients\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Autograd module loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7f17d-b4c9-4387-a5e9-3a9a3da8985c",
   "metadata": {},
   "source": [
    "## 2. Basic Gradient Computation {#basic}\n",
    "\n",
    "### Example 1: Simple Function f(x) = x²\n",
    "\n",
    "Let's compute the gradient of f(x) = x² at x = 3.\n",
    "\n",
    "Mathematically: df/dx = 2x, so at x=3, df/dx = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8096cd9e-2695-47d1-b227-726eb8eb0e2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m x = Tensor([\u001b[32m3.0\u001b[39m], requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m y = \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mx = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.data\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33my = f(x) = x² = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.data\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:326\u001b[39m, in \u001b[36mTensor.__pow__\u001b[39m\u001b[34m(self, power)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__pow__\u001b[39m(\u001b[38;5;28mself\u001b[39m, power: Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m]) -> \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    319\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[33;03m    Raise tensor to a power with gradient tracking.\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    324\u001b[39m \u001b[33;03m        ∂y/∂x = n * x^(n-1) (power rule from calculus)\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     out = \u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_children\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_op\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpow(\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpower\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m out.requires_grad:\n\u001b[32m    334\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_backward\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:84\u001b[39m, in \u001b[36mTensor.__init__\u001b[39m\u001b[34m(self, data, requires_grad, _children, _op)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.grad: Optional[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.grad_fn: Optional[Callable] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28mself\u001b[39m._prev: Set[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_children\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m._op = _op\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'Tensor'"
     ]
    }
   ],
   "source": [
    "# Create tensor with gradient tracking\n",
    "x = Tensor([3.0], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "y = x ** 2\n",
    "print(f\"x = {x.data}\")\n",
    "print(f\"y = f(x) = x² = {y.data}\")\n",
    "\n",
    "# Backward pass\n",
    "y.backward()\n",
    "\n",
    "# Check gradient\n",
    "print(f\"dy/dx = {x.grad.data}\")\n",
    "print(f\"Expected: {2 * 3.0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96664248-4557-4e7d-a409-da7e1158dfca",
   "metadata": {},
   "source": [
    "### Example 2: Multivariable Function\n",
    "\n",
    "Let's compute gradients for f(x, y) = x*y + x²\n",
    "\n",
    "- ∂f/∂x = y + 2x\n",
    "- ∂f/∂y = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01d72771-e642-4428-89b7-0dc718944bbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m y = Tensor([\u001b[32m3.0\u001b[39m], requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Compute function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m z = \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m + x ** \u001b[32m2\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m      9\u001b[39m z.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:266\u001b[39m, in \u001b[36mTensor.__mul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[33;03mMultiply tensors with gradient tracking.\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    263\u001b[39m \u001b[33;03m    ∂y/∂b = a (multiply incoming gradient by other input)\u001b[39;00m\n\u001b[32m    264\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    265\u001b[39m other = other \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Tensor) \u001b[38;5;28;01melse\u001b[39;00m Tensor(other)\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m out = \u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_children\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_op\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmul\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out.requires_grad:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_backward\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:84\u001b[39m, in \u001b[36mTensor.__init__\u001b[39m\u001b[34m(self, data, requires_grad, _children, _op)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.grad: Optional[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.grad_fn: Optional[Callable] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28mself\u001b[39m._prev: Set[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_children\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m._op = _op\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'Tensor'"
     ]
    }
   ],
   "source": [
    "# Create variables\n",
    "x = Tensor([2.0], requires_grad=True)\n",
    "y = Tensor([3.0], requires_grad=True)\n",
    "\n",
    "# Compute function\n",
    "z = x * y + x ** 2\n",
    "\n",
    "# Backward pass\n",
    "z.backward()\n",
    "\n",
    "print(f\"z = x*y + x² = {z.data}\")\n",
    "print(f\"∂z/∂x = {x.grad.data} (expected: y + 2x = 3 + 4 = 7)\")\n",
    "print(f\"∂z/∂y = {y.grad.data} (expected: x = 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226230f5-620c-4e5f-aeb1-0dc236654d82",
   "metadata": {},
   "source": [
    "### Example 3: Vector Operations\n",
    "\n",
    "Autograd works seamlessly with vectors and matrices!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "334f4132-f8d0-41c1-8ce4-1879e0fd6b27",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m x = Tensor([\u001b[32m1.0\u001b[39m, \u001b[32m2.0\u001b[39m, \u001b[32m3.0\u001b[39m, \u001b[32m4.0\u001b[39m], requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Compute sum of squares\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m y = (\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m).sum()\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m      8\u001b[39m y.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:326\u001b[39m, in \u001b[36mTensor.__pow__\u001b[39m\u001b[34m(self, power)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__pow__\u001b[39m(\u001b[38;5;28mself\u001b[39m, power: Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m]) -> \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    319\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[33;03m    Raise tensor to a power with gradient tracking.\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    324\u001b[39m \u001b[33;03m        ∂y/∂x = n * x^(n-1) (power rule from calculus)\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     out = \u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_children\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_op\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpow(\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpower\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m out.requires_grad:\n\u001b[32m    334\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_backward\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:84\u001b[39m, in \u001b[36mTensor.__init__\u001b[39m\u001b[34m(self, data, requires_grad, _children, _op)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.grad: Optional[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.grad_fn: Optional[Callable] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28mself\u001b[39m._prev: Set[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_children\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m._op = _op\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'Tensor'"
     ]
    }
   ],
   "source": [
    "# Vector input\n",
    "x = Tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "\n",
    "# Compute sum of squares\n",
    "y = (x ** 2).sum()\n",
    "\n",
    "# Backward pass\n",
    "y.backward()\n",
    "\n",
    "print(f\"x = {x.data}\")\n",
    "print(f\"y = Σx² = {y.data}\")\n",
    "print(f\"∂y/∂x = {x.grad.data}\")\n",
    "print(f\"Expected: 2x = {2 * x.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29fb905-3324-45bc-aa2f-ce16abec0cbd",
   "metadata": {},
   "source": [
    "## 3. Chain Rule in Action {#chain}\n",
    "\n",
    "The chain rule is fundamental to backpropagation: **df/dx = (df/dy) × (dy/dx)**\n",
    "\n",
    "### Example 1: Simple Chain f(x) = (x²)²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a21fb555-295c-4b5e-8543-01d12c449df2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m x = Tensor([\u001b[32m2.0\u001b[39m], requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Build chain\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m y = \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m      \u001b[38;5;66;03m# y = x² = 4\u001b[39;00m\n\u001b[32m      5\u001b[39m z = y ** \u001b[32m2\u001b[39m      \u001b[38;5;66;03m# z = y² = 16\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33my = x² = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.data\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:326\u001b[39m, in \u001b[36mTensor.__pow__\u001b[39m\u001b[34m(self, power)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__pow__\u001b[39m(\u001b[38;5;28mself\u001b[39m, power: Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m]) -> \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    319\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[33;03m    Raise tensor to a power with gradient tracking.\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    324\u001b[39m \u001b[33;03m        ∂y/∂x = n * x^(n-1) (power rule from calculus)\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     out = \u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_children\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_op\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpow(\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpower\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m out.requires_grad:\n\u001b[32m    334\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_backward\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:84\u001b[39m, in \u001b[36mTensor.__init__\u001b[39m\u001b[34m(self, data, requires_grad, _children, _op)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.grad: Optional[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.grad_fn: Optional[Callable] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28mself\u001b[39m._prev: Set[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_children\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m._op = _op\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'Tensor'"
     ]
    }
   ],
   "source": [
    "x = Tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Build chain\n",
    "y = x ** 2      # y = x² = 4\n",
    "z = y ** 2      # z = y² = 16\n",
    "\n",
    "print(f\"y = x² = {y.data}\")\n",
    "print(f\"z = y² = {z.data}\")\n",
    "\n",
    "# Backward pass\n",
    "z.backward()\n",
    "\n",
    "print(f\"\\nGradient computation:\")\n",
    "print(f\"dz/dy = 2y = {2 * y.data}\")\n",
    "print(f\"dy/dx = 2x = {2 * x.data}\")\n",
    "print(f\"dz/dx = (dz/dy)(dy/dx) = {x.grad.data}\")\n",
    "print(f\"Expected: 2*4 * 2*2 = {2 * 4.0 * 2 * 2.0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a88fd9-e42d-48b3-8809-132a8544df49",
   "metadata": {},
   "source": [
    "### Example 2: Complex Chain f(x) = (x² + 1)³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87847429-7cde-4987-b416-471d8c4728fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m x = Tensor([\u001b[32m2.0\u001b[39m], requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m y = \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m + \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# y = 5\u001b[39;00m\n\u001b[32m      4\u001b[39m z = y ** \u001b[32m3\u001b[39m      \u001b[38;5;66;03m# z = 125\u001b[39;00m\n\u001b[32m      6\u001b[39m z.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:326\u001b[39m, in \u001b[36mTensor.__pow__\u001b[39m\u001b[34m(self, power)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__pow__\u001b[39m(\u001b[38;5;28mself\u001b[39m, power: Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m]) -> \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    319\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[33;03m    Raise tensor to a power with gradient tracking.\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    324\u001b[39m \u001b[33;03m        ∂y/∂x = n * x^(n-1) (power rule from calculus)\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     out = \u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_children\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_op\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpow(\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpower\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m out.requires_grad:\n\u001b[32m    334\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_backward\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:84\u001b[39m, in \u001b[36mTensor.__init__\u001b[39m\u001b[34m(self, data, requires_grad, _children, _op)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.grad: Optional[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.grad_fn: Optional[Callable] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28mself\u001b[39m._prev: Set[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_children\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m._op = _op\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'Tensor'"
     ]
    }
   ],
   "source": [
    "x = Tensor([2.0], requires_grad=True)\n",
    "\n",
    "y = x ** 2 + 1  # y = 5\n",
    "z = y ** 3      # z = 125\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(f\"f(x) = (x² + 1)³\")\n",
    "print(f\"f({x.data[0]}) = {z.data}\")\n",
    "print(f\"\\ndf/dx = 3(x²+1)² × 2x\")\n",
    "print(f\"      = 3(5)² × 4\")\n",
    "print(f\"      = {x.grad.data[0]}\")\n",
    "print(f\"Expected: {3 * 25 * 4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d04c5f-8a5d-4db1-900d-26d37b1f28cd",
   "metadata": {},
   "source": [
    "### Example 3: Multiple Paths (Gradient Accumulation)\n",
    "\n",
    "When a variable appears multiple times, gradients accumulate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50f5d482-6688-4858-a1f0-954047faf02d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m x = Tensor([\u001b[32m3.0\u001b[39m], requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# x appears in two paths\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m y1 = \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m     \u001b[38;5;66;03m# Path 1: dy1/dx = 2x = 6\u001b[39;00m\n\u001b[32m      5\u001b[39m y2 = x * \u001b[32m3\u001b[39m      \u001b[38;5;66;03m# Path 2: dy2/dx = 3\u001b[39;00m\n\u001b[32m      6\u001b[39m z = y1 + y2     \u001b[38;5;66;03m# Combine paths\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:326\u001b[39m, in \u001b[36mTensor.__pow__\u001b[39m\u001b[34m(self, power)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__pow__\u001b[39m(\u001b[38;5;28mself\u001b[39m, power: Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m]) -> \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    319\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[33;03m    Raise tensor to a power with gradient tracking.\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    324\u001b[39m \u001b[33;03m        ∂y/∂x = n * x^(n-1) (power rule from calculus)\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     out = \u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_children\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_op\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpow(\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpower\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m out.requires_grad:\n\u001b[32m    334\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_backward\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:84\u001b[39m, in \u001b[36mTensor.__init__\u001b[39m\u001b[34m(self, data, requires_grad, _children, _op)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.grad: Optional[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.grad_fn: Optional[Callable] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28mself\u001b[39m._prev: Set[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_children\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m._op = _op\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'Tensor'"
     ]
    }
   ],
   "source": [
    "x = Tensor([3.0], requires_grad=True)\n",
    "\n",
    "# x appears in two paths\n",
    "y1 = x ** 2     # Path 1: dy1/dx = 2x = 6\n",
    "y2 = x * 3      # Path 2: dy2/dx = 3\n",
    "z = y1 + y2     # Combine paths\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(f\"z = x² + 3x\")\n",
    "print(f\"dz/dx = 2x + 3 = {x.grad.data[0]}\")\n",
    "print(f\"Expected: {2*3 + 3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6afb97c-fd63-461f-be7f-5a2d7b4cb70b",
   "metadata": {},
   "source": [
    "## 4. Computational Graphs {#graphs}\n",
    "\n",
    "Let's visualize how computational graphs work.\n",
    "\n",
    "### Example: Building a Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c7492d4-83ba-4774-8589-fac0fe317b50",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m x = Tensor([\u001b[32m2.0\u001b[39m], requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m y = Tensor([\u001b[32m3.0\u001b[39m], requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m a = \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m        \u001b[38;5;66;03m# a = 6\u001b[39;00m\n\u001b[32m      6\u001b[39m b = a + x ** \u001b[32m2\u001b[39m   \u001b[38;5;66;03m# b = 10\u001b[39;00m\n\u001b[32m      7\u001b[39m c = b * \u001b[32m2\u001b[39m        \u001b[38;5;66;03m# c = 20\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:266\u001b[39m, in \u001b[36mTensor.__mul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[33;03mMultiply tensors with gradient tracking.\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    263\u001b[39m \u001b[33;03m    ∂y/∂b = a (multiply incoming gradient by other input)\u001b[39;00m\n\u001b[32m    264\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    265\u001b[39m other = other \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Tensor) \u001b[38;5;28;01melse\u001b[39;00m Tensor(other)\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m out = \u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_children\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_op\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmul\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out.requires_grad:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_backward\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:84\u001b[39m, in \u001b[36mTensor.__init__\u001b[39m\u001b[34m(self, data, requires_grad, _children, _op)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.grad: Optional[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.grad_fn: Optional[Callable] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28mself\u001b[39m._prev: Set[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_children\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m._op = _op\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'Tensor'"
     ]
    }
   ],
   "source": [
    "# Create computation\n",
    "x = Tensor([2.0], requires_grad=True)\n",
    "y = Tensor([3.0], requires_grad=True)\n",
    "\n",
    "a = x * y        # a = 6\n",
    "b = a + x ** 2   # b = 10\n",
    "c = b * 2        # c = 20\n",
    "\n",
    "print(\"Computational Graph:\")\n",
    "print(f\"x = {x.data[0]}\")\n",
    "print(f\"y = {y.data[0]}\")\n",
    "print(f\"a = x * y = {a.data[0]}\")\n",
    "print(f\"b = a + x² = {b.data[0]}\")\n",
    "print(f\"c = b * 2 = {c.data[0]}\")\n",
    "\n",
    "# Backward pass\n",
    "c.backward()\n",
    "\n",
    "print(f\"\\nGradients:\")\n",
    "print(f\"∂c/∂x = {x.grad.data[0]}\")\n",
    "print(f\"∂c/∂y = {y.grad.data[0]}\")\n",
    "\n",
    "# Manual calculation for verification\n",
    "print(f\"\\nManual calculation:\")\n",
    "print(f\"∂c/∂b = 2\")\n",
    "print(f\"∂b/∂a = 1, ∂b/∂x = 2x = 4\")\n",
    "print(f\"∂a/∂x = y = 3, ∂a/∂y = x = 2\")\n",
    "print(f\"∂c/∂x = 2 * (1*3 + 4) = {2 * (1*3 + 4)}\")\n",
    "print(f\"∂c/∂y = 2 * (1*2) = {2 * 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dff216b-869c-4ee1-8f4d-1e2b3685a34e",
   "metadata": {},
   "source": [
    "### Matrix Operations Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "622f7076-0152-457e-a56f-d502622f595f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m A = Tensor([[\u001b[32m1.0\u001b[39m, \u001b[32m2.0\u001b[39m], [\u001b[32m3.0\u001b[39m, \u001b[32m4.0\u001b[39m]], requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m B = Tensor([[\u001b[32m5.0\u001b[39m, \u001b[32m6.0\u001b[39m], [\u001b[32m7.0\u001b[39m, \u001b[32m8.0\u001b[39m]], requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m C = \u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m       \u001b[38;5;66;03m# Matrix multiply\u001b[39;00m\n\u001b[32m      6\u001b[39m loss = C.sum()  \u001b[38;5;66;03m# Scalar loss\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mA =\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mA.data\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:731\u001b[39m, in \u001b[36mTensor.__matmul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__matmul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    730\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Matrix multiplication operator @ with gradients.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:680\u001b[39m, in \u001b[36mTensor.matmul\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    677\u001b[39m other = other \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Tensor) \u001b[38;5;28;01melse\u001b[39;00m Tensor(other)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m     out = \u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_children\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_op\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmatmul\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    687\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    688\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIncompatible shapes for matmul: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mother.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    689\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:84\u001b[39m, in \u001b[36mTensor.__init__\u001b[39m\u001b[34m(self, data, requires_grad, _children, _op)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.grad: Optional[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.grad_fn: Optional[Callable] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28mself\u001b[39m._prev: Set[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_children\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m._op = _op\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'Tensor'"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication example\n",
    "A = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "B = Tensor([[5.0, 6.0], [7.0, 8.0]], requires_grad=True)\n",
    "\n",
    "C = A @ B       # Matrix multiply\n",
    "loss = C.sum()  # Scalar loss\n",
    "\n",
    "print(f\"A =\\n{A.data}\")\n",
    "print(f\"B =\\n{B.data}\")\n",
    "print(f\"C = A @ B =\\n{C.data}\")\n",
    "print(f\"loss = sum(C) = {loss.data[0]}\")\n",
    "\n",
    "# Backward\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\n∂loss/∂A =\\n{A.grad.data}\")\n",
    "print(f\"∂loss/∂B =\\n{B.grad.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70651f5c-c1cf-48e2-b99c-0b37da93bfde",
   "metadata": {},
   "source": [
    "## 5. Real-World Examples {#examples}\n",
    "\n",
    "### Example 1: Linear Regression\n",
    "\n",
    "Let's implement linear regression from scratch using autograd!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2fa2f4e-81f8-4fc6-84c0-9591c42580cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     19\u001b[39m losses = []\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     y_pred = \u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m + b\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# Compute MSE loss\u001b[39;00m\n\u001b[32m     26\u001b[39m     diff = y_pred - y_true\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:731\u001b[39m, in \u001b[36mTensor.__matmul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__matmul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    730\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Matrix multiplication operator @ with gradients.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:680\u001b[39m, in \u001b[36mTensor.matmul\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    677\u001b[39m other = other \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Tensor) \u001b[38;5;28;01melse\u001b[39;00m Tensor(other)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m     out = \u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_children\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_op\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmatmul\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    687\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    688\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIncompatible shapes for matmul: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mother.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    689\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:84\u001b[39m, in \u001b[36mTensor.__init__\u001b[39m\u001b[34m(self, data, requires_grad, _children, _op)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.grad: Optional[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.grad_fn: Optional[Callable] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28mself\u001b[39m._prev: Set[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_children\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m._op = _op\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'Tensor'"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X_data = np.random.randn(100, 1) * 2\n",
    "y_data = 3 * X_data + 2 + np.random.randn(100, 1) * 0.5\n",
    "\n",
    "# Convert to tensors\n",
    "X = Tensor(X_data, requires_grad=False)\n",
    "y_true = Tensor(y_data, requires_grad=False)\n",
    "\n",
    "# Initialize parameters\n",
    "w = Tensor(np.random.randn(1, 1), requires_grad=True)\n",
    "b = Tensor(np.zeros((1, 1)), requires_grad=True)\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    y_pred = X @ w + b\n",
    "    \n",
    "    # Compute MSE loss\n",
    "    diff = y_pred - y_true\n",
    "    loss = (diff ** 2).mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    w.zero_grad()\n",
    "    b.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient descent step\n",
    "    with no_grad():\n",
    "        w.data -= learning_rate * w.grad.data\n",
    "        b.data -= learning_rate * b.grad.data\n",
    "    \n",
    "    losses.append(loss.data.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.data[0]:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal parameters:\")\n",
    "print(f\"w = {w.data[0,0]:.4f} (true: 3.0)\")\n",
    "print(f\"b = {b.data[0,0]:.4f} (true: 2.0)\")\n",
    "\n",
    "# Plot loss curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot predictions\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_data, y_data, alpha=0.5, label='True data')\n",
    "y_pred_final = X_data @ w.data + b.data\n",
    "plt.plot(X_data, y_pred_final, 'r-', linewidth=2, label='Fitted line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression Result')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b476b7-b7cd-4ae7-9951-6c98b0c51192",
   "metadata": {},
   "source": [
    "### Example 2: Simple Neural Network (2 Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7317da83-ba41-4228-88a4-5c51c472af72",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     26\u001b[39m losses = []\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     h = relu(\u001b[43mX_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m + b1)  \u001b[38;5;66;03m# Hidden layer with ReLU\u001b[39;00m\n\u001b[32m     31\u001b[39m     y_pred = h @ W2 + b2          \u001b[38;5;66;03m# Output layer\u001b[39;00m\n\u001b[32m     33\u001b[39m     \u001b[38;5;66;03m# Binary cross-entropy approximation with MSE\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:731\u001b[39m, in \u001b[36mTensor.__matmul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__matmul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    730\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Matrix multiplication operator @ with gradients.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:680\u001b[39m, in \u001b[36mTensor.matmul\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    677\u001b[39m other = other \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Tensor) \u001b[38;5;28;01melse\u001b[39;00m Tensor(other)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m     out = \u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_children\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_op\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmatmul\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    687\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    688\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIncompatible shapes for matmul: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mother.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    689\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:84\u001b[39m, in \u001b[36mTensor.__init__\u001b[39m\u001b[34m(self, data, requires_grad, _children, _op)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.grad: Optional[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.grad_fn: Optional[Callable] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28mself\u001b[39m._prev: Set[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_children\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m._op = _op\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'Tensor'"
     ]
    }
   ],
   "source": [
    "# Generate nonlinear data\n",
    "np.random.seed(42)\n",
    "X_data = np.random.randn(200, 2)\n",
    "y_data = (X_data[:, 0]**2 + X_data[:, 1]**2 > 1).astype(float).reshape(-1, 1)\n",
    "\n",
    "# Split into train/test\n",
    "X_train = Tensor(X_data[:150], requires_grad=False)\n",
    "y_train = Tensor(y_data[:150], requires_grad=False)\n",
    "X_test = Tensor(X_data[150:], requires_grad=False)\n",
    "y_test = Tensor(y_data[150:], requires_grad=False)\n",
    "\n",
    "# Initialize network parameters\n",
    "np.random.seed(42)\n",
    "W1 = Tensor(np.random.randn(2, 10) * 0.1, requires_grad=True)\n",
    "b1 = Tensor(np.zeros((1, 10)), requires_grad=True)\n",
    "W2 = Tensor(np.random.randn(10, 1) * 0.1, requires_grad=True)\n",
    "b2 = Tensor(np.zeros((1, 1)), requires_grad=True)\n",
    "\n",
    "# Simple ReLU implementation\n",
    "def relu(x):\n",
    "    return x * (x.data > 0)\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.1\n",
    "epochs = 200\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    h = relu(X_train @ W1 + b1)  # Hidden layer with ReLU\n",
    "    y_pred = h @ W2 + b2          # Output layer\n",
    "    \n",
    "    # Binary cross-entropy approximation with MSE\n",
    "    loss = ((y_pred - y_train) ** 2).mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    for param in [W1, b1, W2, b2]:\n",
    "        param.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    with no_grad():\n",
    "        for param in [W1, b1, W2, b2]:\n",
    "            param.data -= learning_rate * param.grad.data\n",
    "    \n",
    "    losses.append(loss.data.item())\n",
    "    \n",
    "    if (epoch + 1) % 40 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.data[0]:.4f}\")\n",
    "\n",
    "# Test accuracy\n",
    "with no_grad():\n",
    "    h_test = relu(X_test @ W1 + b1)\n",
    "    y_pred_test = h_test @ W2 + b2\n",
    "    predictions = (y_pred_test.data > 0.5).astype(float)\n",
    "    accuracy = (predictions == y_test.data).mean()\n",
    "    print(f\"\\nTest Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Loss curve\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "# Training data\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X_data[:150, 0], X_data[:150, 1], c=y_data[:150], cmap='RdYlBu', alpha=0.6)\n",
    "plt.title('Training Data')\n",
    "plt.colorbar()\n",
    "plt.grid(True)\n",
    "\n",
    "# Test predictions\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(X_data[150:, 0], X_data[150:, 1], c=predictions, cmap='RdYlBu', alpha=0.6)\n",
    "plt.title('Test Predictions')\n",
    "plt.colorbar()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59919e4-97cd-47e5-bb38-a4e9226d907b",
   "metadata": {},
   "source": [
    "## 6. Gradient Checking {#checking}\n",
    "\n",
    "Always verify your gradients with numerical approximation!\n",
    "\n",
    "### Example 1: Simple Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "878caf87-80e6-4a45-ae4b-eac60ae22fc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m x = Tensor([\u001b[32m1.0\u001b[39m, \u001b[32m2.0\u001b[39m, \u001b[32m3.0\u001b[39m], requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Check gradients\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m match, diff = \u001b[43mcheck_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGradients match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMax difference: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:954\u001b[39m, in \u001b[36mcheck_gradients\u001b[39m\u001b[34m(func, tensor, epsilon, tolerance)\u001b[39m\n\u001b[32m    952\u001b[39m \u001b[38;5;66;03m# Compute autograd gradient\u001b[39;00m\n\u001b[32m    953\u001b[39m tensor.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    955\u001b[39m output.backward()\n\u001b[32m    956\u001b[39m autograd_grad = tensor.grad.data.copy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mf\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mf\u001b[39m(x):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ((\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m) * \u001b[32m3\u001b[39m + x).sum()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:326\u001b[39m, in \u001b[36mTensor.__pow__\u001b[39m\u001b[34m(self, power)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__pow__\u001b[39m(\u001b[38;5;28mself\u001b[39m, power: Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m]) -> \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    319\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[33;03m    Raise tensor to a power with gradient tracking.\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    324\u001b[39m \u001b[33;03m        ∂y/∂x = n * x^(n-1) (power rule from calculus)\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     out = \u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_children\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_op\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpow(\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpower\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m out.requires_grad:\n\u001b[32m    334\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_backward\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:84\u001b[39m, in \u001b[36mTensor.__init__\u001b[39m\u001b[34m(self, data, requires_grad, _children, _op)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.grad: Optional[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.grad_fn: Optional[Callable] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28mself\u001b[39m._prev: Set[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_children\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m._op = _op\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'Tensor'"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return ((x ** 2) * 3 + x).sum()\n",
    "\n",
    "x = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Check gradients\n",
    "match, diff = check_gradients(f, x, epsilon=1e-5, tolerance=1e-5)\n",
    "\n",
    "print(f\"Gradients match: {match}\")\n",
    "print(f\"Max difference: {diff:.2e}\")\n",
    "\n",
    "# Compute both gradients for comparison\n",
    "x.zero_grad()\n",
    "y = f(x)\n",
    "y.backward()\n",
    "autograd_grad = x.grad.data.copy()\n",
    "\n",
    "from semester1.lab2_autograd.autograd import numerical_gradient\n",
    "numerical_grad = numerical_gradient(f, x)\n",
    "\n",
    "print(f\"\\nAutograd gradient:  {autograd_grad}\")\n",
    "print(f\"Numerical gradient: {numerical_grad}\")\n",
    "print(f\"Difference:         {autograd_grad - numerical_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3d58d1-9615-4aa5-891a-be4c9026a418",
   "metadata": {},
   "source": [
    "### Example 2: Matrix Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a179df6-24ae-4726-a8b6-0faa2003d4b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (C ** \u001b[32m2\u001b[39m).sum()\n\u001b[32m      6\u001b[39m A = Tensor([[\u001b[32m1.0\u001b[39m, \u001b[32m2.0\u001b[39m], [\u001b[32m3.0\u001b[39m, \u001b[32m4.0\u001b[39m]], requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m match, diff = \u001b[43mcheck_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatrix_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMatrix gradients match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMax difference: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:954\u001b[39m, in \u001b[36mcheck_gradients\u001b[39m\u001b[34m(func, tensor, epsilon, tolerance)\u001b[39m\n\u001b[32m    952\u001b[39m \u001b[38;5;66;03m# Compute autograd gradient\u001b[39;00m\n\u001b[32m    953\u001b[39m tensor.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    955\u001b[39m output.backward()\n\u001b[32m    956\u001b[39m autograd_grad = tensor.grad.data.copy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmatrix_func\u001b[39m\u001b[34m(A)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmatrix_func\u001b[39m(A):\n\u001b[32m      2\u001b[39m     B = Tensor([[\u001b[32m2.0\u001b[39m, \u001b[32m3.0\u001b[39m], [\u001b[32m4.0\u001b[39m, \u001b[32m5.0\u001b[39m]], requires_grad=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     C = \u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (C ** \u001b[32m2\u001b[39m).sum()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:731\u001b[39m, in \u001b[36mTensor.__matmul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__matmul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    730\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Matrix multiplication operator @ with gradients.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:680\u001b[39m, in \u001b[36mTensor.matmul\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    677\u001b[39m other = other \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Tensor) \u001b[38;5;28;01melse\u001b[39;00m Tensor(other)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m     out = \u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_children\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_op\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmatmul\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    687\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    688\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIncompatible shapes for matmul: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mother.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    689\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:84\u001b[39m, in \u001b[36mTensor.__init__\u001b[39m\u001b[34m(self, data, requires_grad, _children, _op)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.grad: Optional[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.grad_fn: Optional[Callable] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28mself\u001b[39m._prev: Set[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_children\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m._op = _op\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'Tensor'"
     ]
    }
   ],
   "source": [
    "def matrix_func(A):\n",
    "    B = Tensor([[2.0, 3.0], [4.0, 5.0]], requires_grad=False)\n",
    "    C = A @ B\n",
    "    return (C ** 2).sum()\n",
    "\n",
    "A = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "\n",
    "match, diff = check_gradients(matrix_func, A)\n",
    "\n",
    "print(f\"Matrix gradients match: {match}\")\n",
    "print(f\"Max difference: {diff:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb038639-0700-47ce-a33e-134b4b5821bc",
   "metadata": {},
   "source": [
    "## 7. Common Pitfalls {#pitfalls}\n",
    "\n",
    "### Pitfall 1: Forgetting to Zero Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64df4d71-f3b0-4abf-934c-dfc637832a23",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m x = Tensor([\u001b[32m2.0\u001b[39m], requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# First computation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m y1 = \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m      5\u001b[39m y1.backward()\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFirst gradient: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.grad.data\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:326\u001b[39m, in \u001b[36mTensor.__pow__\u001b[39m\u001b[34m(self, power)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__pow__\u001b[39m(\u001b[38;5;28mself\u001b[39m, power: Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m]) -> \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    319\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[33;03m    Raise tensor to a power with gradient tracking.\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    324\u001b[39m \u001b[33;03m        ∂y/∂x = n * x^(n-1) (power rule from calculus)\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     out = \u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_children\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_op\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpow(\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpower\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m out.requires_grad:\n\u001b[32m    334\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_backward\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:84\u001b[39m, in \u001b[36mTensor.__init__\u001b[39m\u001b[34m(self, data, requires_grad, _children, _op)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.grad: Optional[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.grad_fn: Optional[Callable] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28mself\u001b[39m._prev: Set[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_children\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m._op = _op\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'Tensor'"
     ]
    }
   ],
   "source": [
    "x = Tensor([2.0], requires_grad=True)\n",
    "\n",
    "# First computation\n",
    "y1 = x ** 2\n",
    "y1.backward()\n",
    "print(f\"First gradient: {x.grad.data}\")\n",
    "\n",
    "# Second computation WITHOUT zero_grad\n",
    "y2 = x * 3\n",
    "y2.backward()\n",
    "print(f\"Accumulated gradient: {x.grad.data}\")  # Oops! Accumulated!\n",
    "\n",
    "# Correct way\n",
    "x.zero_grad()\n",
    "y2 = x * 3\n",
    "y2.backward()\n",
    "print(f\"Correct gradient: {x.grad.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb209bc-3b0c-45b6-96c6-cdd3200b3ae3",
   "metadata": {},
   "source": [
    "### Pitfall 2: Modifying Tensors In-Place During Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e930ed12-d720-417e-8b4d-ea62212ab07b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# x.data *= 2  # DON'T DO THIS!\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# CORRECT: Create new tensor\u001b[39;00m\n\u001b[32m      6\u001b[39m x = Tensor([\u001b[32m1.0\u001b[39m, \u001b[32m2.0\u001b[39m, \u001b[32m3.0\u001b[39m], requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m y = \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m  \u001b[38;5;66;03m# Creates new tensor\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:266\u001b[39m, in \u001b[36mTensor.__mul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[33;03mMultiply tensors with gradient tracking.\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    263\u001b[39m \u001b[33;03m    ∂y/∂b = a (multiply incoming gradient by other input)\u001b[39;00m\n\u001b[32m    264\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    265\u001b[39m other = other \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Tensor) \u001b[38;5;28;01melse\u001b[39;00m Tensor(other)\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m out = \u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_children\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_op\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmul\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out.requires_grad:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_backward\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:84\u001b[39m, in \u001b[36mTensor.__init__\u001b[39m\u001b[34m(self, data, requires_grad, _children, _op)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.grad: Optional[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.grad_fn: Optional[Callable] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28mself\u001b[39m._prev: Set[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_children\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m._op = _op\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'Tensor'"
     ]
    }
   ],
   "source": [
    "# WRONG: In-place modification breaks gradient\n",
    "x = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "# x.data *= 2  # DON'T DO THIS!\n",
    "\n",
    "# CORRECT: Create new tensor\n",
    "x = Tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x * 2  # Creates new tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5dfff1-5a61-4a37-877d-3f0f3d94de0c",
   "metadata": {},
   "source": [
    "### Pitfall 3: Calling backward() on Non-Scalar Without Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ef4fb85-d339-404d-a8bd-8413944c7494",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m x = Tensor([[\u001b[32m1.0\u001b[39m, \u001b[32m2.0\u001b[39m], [\u001b[32m3.0\u001b[39m, \u001b[32m4.0\u001b[39m]], requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m y = \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# WRONG: y is not a scalar\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:266\u001b[39m, in \u001b[36mTensor.__mul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[33;03mMultiply tensors with gradient tracking.\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    263\u001b[39m \u001b[33;03m    ∂y/∂b = a (multiply incoming gradient by other input)\u001b[39;00m\n\u001b[32m    264\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    265\u001b[39m other = other \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Tensor) \u001b[38;5;28;01melse\u001b[39;00m Tensor(other)\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m out = \u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_children\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_op\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmul\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out.requires_grad:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_backward\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:84\u001b[39m, in \u001b[36mTensor.__init__\u001b[39m\u001b[34m(self, data, requires_grad, _children, _op)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.grad: Optional[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.grad_fn: Optional[Callable] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28mself\u001b[39m._prev: Set[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_children\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m._op = _op\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'Tensor'"
     ]
    }
   ],
   "source": [
    "x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "y = x * 2\n",
    "\n",
    "# WRONG: y is not a scalar\n",
    "try:\n",
    "    y.backward()\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# CORRECT: Reduce to scalar\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "print(f\"Gradient shape: {x.grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099f423a-d3dd-4be3-acc3-e6c590255025",
   "metadata": {},
   "source": [
    "### Pitfall 4: Using Operations Inside no_grad When You Need Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b2835a2-b692-42f6-854d-9687162e2791",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# WRONG: Gradients won't be computed\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     y = \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# y.backward()  # This would fail!\u001b[39;00m\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# CORRECT: Don't use no_grad for forward pass that needs gradients\u001b[39;00m\n\u001b[32m      9\u001b[39m x = Tensor([\u001b[32m2.0\u001b[39m], requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:326\u001b[39m, in \u001b[36mTensor.__pow__\u001b[39m\u001b[34m(self, power)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__pow__\u001b[39m(\u001b[38;5;28mself\u001b[39m, power: Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m]) -> \u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    319\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[33;03m    Raise tensor to a power with gradient tracking.\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    324\u001b[39m \u001b[33;03m        ∂y/∂x = n * x^(n-1) (power rule from calculus)\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     out = \u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_children\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_op\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpow(\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpower\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m out.requires_grad:\n\u001b[32m    334\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_backward\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NNframework/nn-from-scratch/semester1/lab2_autograd/notebooks/../../../semester1/lab2_autograd/autograd.py:84\u001b[39m, in \u001b[36mTensor.__init__\u001b[39m\u001b[34m(self, data, requires_grad, _children, _op)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.grad: Optional[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.grad_fn: Optional[Callable] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28mself\u001b[39m._prev: Set[\u001b[33m\"\u001b[39m\u001b[33mTensor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_children\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m._op = _op\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'Tensor'"
     ]
    }
   ],
   "source": [
    "x = Tensor([2.0], requires_grad=True)\n",
    "\n",
    "# WRONG: Gradients won't be computed\n",
    "with no_grad():\n",
    "    y = x ** 2\n",
    "    # y.backward()  # This would fail!\n",
    "\n",
    "# CORRECT: Don't use no_grad for forward pass that needs gradients\n",
    "x = Tensor([2.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "print(f\"Gradient: {x.grad.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3074795-eafe-4af4-a43a-1af1d030552f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
